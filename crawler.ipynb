{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawler",
      "provenance": [],
      "collapsed_sections": [
        "dYtV3KK1DkGd",
        "Jxft1w_vDyo5",
        "rq36vH2pECp_",
        "WfCyKRa5EJck",
        "x5yi8Ik3EWL9",
        "vCLNkCP3Ecqr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN72BruZu865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b460243-2b1a-483a-fa89-f09db253d697"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V4xqyoBE5QZ"
      },
      "source": [
        "# TODO\n",
        "# code crashing when folder already exist because of makedirs\n",
        "# create folders dinamicaly to run with different depths without intervention\n",
        "# save metrics on file \n",
        "\n",
        "ROOT_URL = '<url inicial aqui. Ex. www.teste.com.br>'\n",
        "# ROOT_DIR = './'\n",
        "ROOT_DIR = 'diretório/onde/você/vai/salvar'\n",
        "MAX_DEPTH = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYtV3KK1DkGd"
      },
      "source": [
        "## UrlManager\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1wEbUmTDoRS"
      },
      "source": [
        "from urllib.parse import urlparse\n",
        "import re\n",
        "\n",
        "class UrlManager:\n",
        "  def __init__(self, root_url):\n",
        "    self.__root_url = root_url\n",
        "    self.__url_parsed = urlparse(root_url)\n",
        "  \n",
        "  @staticmethod\n",
        "  def is_same_domain(root_url, url):\n",
        "    root_domain = urlparse(root_url).netloc\n",
        "    url_domain = urlparse(url).netloc\n",
        "    return root_domain == url_domain\n",
        "\n",
        "  @staticmethod\n",
        "  def get_name_from_url(url):\n",
        "    # url = re.compile(r\"https?://(www\\.)?\")\n",
        "    name_regex = re.compile(r\"https?://(www\\.)?\")\n",
        "    return name_regex.sub('', url).strip().strip('/')\n",
        "  \n",
        "  @staticmethod\n",
        "  def get_domain(url):\n",
        "    return urlparse(url).netloc\n",
        "  \n",
        "  @staticmethod\n",
        "  def get_html_info(url):\n",
        "    parsed = urlparse(url)\n",
        "    root = UrlManager.get_name_from_url(parsed.scheme+'://'+parsed.netloc)\n",
        "    path = parsed.path\n",
        "    name = path.split('/')[-1]\n",
        "    if len(name) and name.split('.')[-1] in ['html','htm']:\n",
        "      return {\n",
        "        \"root\": root,\n",
        "        \"path\": '/'.join(path.split('/')[:-1]),\n",
        "        \"filename\": name,\n",
        "      }\n",
        "    if len(name):\n",
        "      return {\n",
        "        \"root\": root,\n",
        "        \"path\": path,\n",
        "        \"filename\": \"index.html\",\n",
        "      }\n",
        "    return {\n",
        "      \"root\": root,\n",
        "      \"path\": '',\n",
        "      \"filename\": \"index.html\",\n",
        "    }\n",
        "    \n",
        "  @staticmethod\n",
        "  def get_name_of_file(url):\n",
        "    parsed = urlparse(url)\n",
        "    path = parsed.path\n",
        "    last_name = path.split('/')[-1]\n",
        "    return last_name\n",
        "  \n",
        "  @staticmethod\n",
        "  def has_query_strings(url):\n",
        "    parsed = urlparse(url)\n",
        "    return parsed.query != ''\n",
        "  \n",
        "  @staticmethod\n",
        "  def sanitize_links(root_url, links):\n",
        "    if type(links) == 'string':\n",
        "      links = [links]\n",
        "    \n",
        "    sanitized_links = []\n",
        "    for link in links:\n",
        "      if len(link) == 0:\n",
        "        continue\n",
        "      elif link[0] == 'h':\n",
        "        sanitized_links.append(link)\n",
        "      elif link[0] == '/':\n",
        "        sanitized_links.append(root_url + link)\n",
        "      else:\n",
        "        continue\n",
        "      parsed_url = urlparse(sanitized_links[-1])\n",
        "      if len(parsed_url) == 0:\n",
        "        sanitized_links.pop()\n",
        "    \n",
        "    return sanitized_links\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxft1w_vDyo5"
      },
      "source": [
        "## Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QSiQOkOD6mL"
      },
      "source": [
        "import os\n",
        "\n",
        "RELATIVE_PATH = 'logs'\n",
        "\n",
        "class Logger:\n",
        "  def __init__(self, class_name, log_filename):\n",
        "    try:\n",
        "      os.makedirs(ROOT_DIR + RELATIVE_PATH)\n",
        "    except:\n",
        "      pass\n",
        "    self.__file = open(ROOT_DIR + RELATIVE_PATH + '/' + log_filename + '.txt', 'w+')\n",
        "    self.__class_name = class_name\n",
        "    self.__prefix = '[{}]: '.format(class_name)\n",
        "  \n",
        "  def log(self, message):\n",
        "    self.__file.write(self.__prefix + message + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq36vH2pECp_"
      },
      "source": [
        "## Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7F6hIw9EIGR"
      },
      "source": [
        "import time\n",
        "from enum import Enum\n",
        "\n",
        "COUNTER_IMAGES_DOWNLOADED = 'COUNTER_IMAGES_DOWNLOADED'\n",
        "COUNTER_HTML_DOWNLOADED = 'COUNTER_HTML_DOWNLOADED'\n",
        "COUNTER_JS_DOWNLOADED = 'COUNTER_JS_DOWNLOADED'\n",
        "COUNTER_CSS_DOWNLOADED = 'COUNTER_CSS_DOWNLOADED'\n",
        "COUNTER_DIFFERENT_DOMAINS_AVOIDED = 'COUNTER_DIFFERENT_DOMAINS_AVOIDED'\n",
        "COUNTER_QUERY_STRINGS_AVOIDED = 'COUNTER_QUERY_STRINGS_AVOIDED'\n",
        "COUNTER_URLS_VISITED = 'COUNTER_URLS_VISITED'\n",
        "COUNTER_FAILED_REQUESTS = 'COUNTER_FAILED_REQUESTS'\n",
        "\n",
        "GAUGE_QUEUE_LENGTH = 'GAUGE_QUEUE_LENGTH'\n",
        "\n",
        "class Metrics:\n",
        "  _instance = None\n",
        "\n",
        "  def __init__(self):\n",
        "    self.__start = None\n",
        "    self.__end = None\n",
        "    self.counters = {}\n",
        "    self.gauges = {}\n",
        "\n",
        "  @classmethod\n",
        "  def instance(cls):\n",
        "    if cls._instance is None:\n",
        "      cls._instance = cls()\n",
        "    return cls._instance\n",
        "  \n",
        "  def start_timer(self):\n",
        "    if self.__start:\n",
        "      print('Timer was started before. Reseting timer with the current time...')\n",
        "    self.__start = time.time()\n",
        "  \n",
        "  def stop_timer(self):\n",
        "    if not self.__start:\n",
        "      print('Cannot stop timer before it was started')\n",
        "      return\n",
        "    self.__end = time.time()\n",
        "  \n",
        "  def increment_counter(self, counter_name):\n",
        "    self.counters[counter_name] = self.counters.get(counter_name, 0) + 1\n",
        "  \n",
        "  def set_gauge(self, gauge_name, value):\n",
        "    if self.gauges.get(gauge_name) == None:\n",
        "      self.gauges[gauge_name] = {\n",
        "          \"maximum\": value,\n",
        "          \"current\": value,\n",
        "          \"minimum\": value,\n",
        "      }\n",
        "    \n",
        "    self.gauges[gauge_name][\"current\"] = value\n",
        "    self.gauges[gauge_name][\"minimum\"] = min(self.gauges[gauge_name][\"minimum\"], value)\n",
        "    self.gauges[gauge_name][\"maximum\"] = max(self.gauges[gauge_name][\"maximum\"], value)\n",
        "  \n",
        "  def show_statistics(self):\n",
        "    time = None\n",
        "    if self.__start and self.__end:\n",
        "      time = self.__end - self.__start\n",
        "    else:\n",
        "      time = 'Timer not used'\n",
        "    print('Running time: ', time)\n",
        "    \n",
        "    for key in self.counters.keys():\n",
        "      print('{}: {}'.format(key, self.counters[key]))\n",
        "    \n",
        "    for key in self.gauges.keys():\n",
        "      print('{}: min({}) max({}) curr({})'.format(key, self.gauges[key][\"minimum\"], self.gauges[key][\"maximum\"], self.gauges[key][\"current\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfCyKRa5EJck"
      },
      "source": [
        "## FileManager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgpigHaQERLS"
      },
      "source": [
        "import os\n",
        "from enum import Enum\n",
        "# from UrlManager import UrlManager\n",
        "import shutil\n",
        "# from Logger import Logger\n",
        "# from Metrics import *\n",
        "\n",
        "'''\n",
        "classe responsável por salvar os arquivos adequadamente\n",
        "'''\n",
        "\n",
        "REPLACE_LABEL = 'root_directory'\n",
        "DIR_IMAGES = 'root_directory/assets/images'\n",
        "DIR_CSS = 'root_directory/assets/css'\n",
        "DIR_JS = 'root_directory/assets/js'\n",
        "DIR_HTML = 'root_directory/html'\n",
        "\n",
        "class FileType(Enum):\n",
        "  HTML = 0\n",
        "  CSS = 1\n",
        "  JS = 2\n",
        "  IMAGE = 3\n",
        "\n",
        "class FileManager:\n",
        "  def __init__(self, root_url, root_path):\n",
        "    self.__root_path = root_path\n",
        "    self.__website_name = UrlManager.get_name_from_url(root_url)\n",
        "    self.__html_folder = self.__create_folder(DIR_HTML)\n",
        "    self.__css_folder = self.__create_folder(DIR_CSS)\n",
        "    self.__js_folder = self.__create_folder(DIR_JS)\n",
        "    self.__images_folder = self.__create_folder(DIR_IMAGES)\n",
        "    self.__html_files_saved = set()\n",
        "    self.__css_files_saved = set()\n",
        "    self.__js_files_saved = set()\n",
        "    self.__images_files_saved = set()\n",
        "    self.logger = Logger('FileManager', 'filemanager')\n",
        "    self.metrics = Metrics.instance()\n",
        "  \n",
        "  def save_file(self, url, content, file_type):\n",
        "    file_saver = self.__get_correct_saver(file_type)\n",
        "    file_saver(url, content)\n",
        "  \n",
        "  def open(self, path, mode):\n",
        "    file = open(self.__root_path+path, mode)\n",
        "    return file\n",
        "\n",
        "  def makedirs(self, path):\n",
        "    try:\n",
        "      os.makedirs(self.__root_path + path)\n",
        "    except (FileExistsError):\n",
        "      self.logger.log(\"[ERRO] cannot create a directory with path {}\".format(path))\n",
        "  \n",
        "  def __create_folder(self, directory):\n",
        "    folder_path = directory.replace(REPLACE_LABEL, self.__website_name)\n",
        "    self.makedirs(folder_path)\n",
        "    return folder_path\n",
        "  \n",
        "  def __get_correct_saver(self, file_type):\n",
        "    savers = {\n",
        "      FileType.HTML: self.__html_saver,\n",
        "      FileType.CSS: self.__css_saver,\n",
        "      FileType.JS: self.__js_saver,\n",
        "      FileType.IMAGE: self.__image_saver,\n",
        "    }\n",
        "    return savers[file_type]\n",
        "\n",
        "  def __html_saver(self, url, content):\n",
        "    if url in self.__html_files_saved:\n",
        "      return\n",
        "    html_info = UrlManager.get_html_info(url)\n",
        "    directory = self.__html_folder + '/' + html_info[\"root\"] + html_info[\"path\"]\n",
        "    self.makedirs(directory)\n",
        "    \n",
        "    try:\n",
        "      file = self.open(directory+'/'+html_info[\"filename\"], 'w+')\n",
        "      file.write(content.text)\n",
        "      file.close()\n",
        "      self.metrics.increment_counter(COUNTER_HTML_DOWNLOADED)\n",
        "    except:\n",
        "      self.logger.log(\"[ERROR] cannot save html {} with name {}\".format(url, html_info[\"filename\"]))\n",
        "    self.__html_files_saved.add(url)\n",
        "  \n",
        "  def __js_saver(self, url, content):\n",
        "    if url in self.__js_files_saved:\n",
        "      return\n",
        "    filename = UrlManager.get_name_of_file(url)\n",
        "    try:\n",
        "      file = self.open(self.__js_folder+'/'+filename, 'w+')\n",
        "      file.write(content.text)\n",
        "      file.close()\n",
        "      self.metrics.increment_counter(COUNTER_JS_DOWNLOADED)\n",
        "    except:\n",
        "      self.logger.log(\"[ERROR] cannot save js {} with name {}\".format(url, filename))\n",
        "    self.__js_files_saved.add(url)\n",
        "    \n",
        "  def __css_saver(self, url, content):\n",
        "    if url in self.__css_files_saved:\n",
        "      return\n",
        "    filename = UrlManager.get_name_of_file(url)\n",
        "    try:\n",
        "      file = self.open(self.__css_folder+'/'+filename, 'w+')\n",
        "      file.write(content.text)\n",
        "      file.close()\n",
        "      self.metrics.increment_counter(COUNTER_CSS_DOWNLOADED)\n",
        "    except:\n",
        "      self.logger.log(\"[ERROR] cannot save css {} with name {}\".format(url, filename))\n",
        "    self.__css_files_saved.add(url)\n",
        "  \n",
        "  def __image_saver(self, url, content):\n",
        "    if url in self.__images_files_saved:\n",
        "      return\n",
        "    filename = UrlManager.get_name_of_file(url)\n",
        "    try:\n",
        "      file = self.open(self.__images_folder+'/'+filename, 'wb')\n",
        "      content.raw.decode_content = True\n",
        "      shutil.copyfileobj(content.raw, file)\n",
        "      file.close()\n",
        "      self.metrics.increment_counter(COUNTER_IMAGES_DOWNLOADED)\n",
        "    except:\n",
        "      self.logger.log(\"[ERROR] cannot save image {} with name {}\".format(url, filename))\n",
        "    self.__images_files_saved.add(url)\n",
        "    del content\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   a = FileManager('www.amazon.com.br')\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5yi8Ik3EWL9"
      },
      "source": [
        "## ParseManager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep1YX8aHEb0O"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "class ParseManager:\n",
        "  @staticmethod\n",
        "  def get_links_to_navigate(requests_response):\n",
        "    html = requests_response.text\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "    all_tags_a = soup.find_all('a', href=True)\n",
        "    links = []\n",
        "    for tag in all_tags_a:\n",
        "      links.append(tag['href'])\n",
        "    return links\n",
        "\n",
        "  @staticmethod\n",
        "  def get_images_urls(requests_response):\n",
        "    html = requests_response.text\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "    all_tags_img = soup.find_all('img', src=True)\n",
        "    srcs = []\n",
        "    for tag in all_tags_img:\n",
        "      srcs.append(tag['src'])\n",
        "    \n",
        "    return srcs\n",
        "\n",
        "  @staticmethod\n",
        "  def get_js_urls(requests_response):\n",
        "    html = requests_response.text\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "    all_tags_script = soup.find_all('script', src=True)\n",
        "    srcs = []\n",
        "    for tag in all_tags_script:\n",
        "      srcs.append(tag['src'])\n",
        "    \n",
        "    return srcs\n",
        "\n",
        "  # [TODO] tem outros tipos alem de stylesheet\n",
        "  @staticmethod\n",
        "  def get_css_urls(requests_response):\n",
        "    html = requests_response.text\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "    all_tags_link = soup.find_all('link', href=True)\n",
        "    srcs = []\n",
        "    for tag in all_tags_link:\n",
        "      if 'stylesheet' in tag['rel']:\n",
        "        srcs.append(tag['href'])\n",
        "    \n",
        "    return srcs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCLNkCP3Ecqr"
      },
      "source": [
        "## RequestsManager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISs_WKtCEhMi"
      },
      "source": [
        "import requests\n",
        "# from Logger import Logger\n",
        "\n",
        "headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'}\n",
        "\n",
        "class RequestsManager:\n",
        "  def __init__(self, root_url):\n",
        "    self.root_url = root_url\n",
        "    self.logger = Logger('RequestsManager', 'failed_requests')\n",
        "    self.metrics = Metrics.instance()\n",
        "\n",
        "  def simple_request(self, url):\n",
        "    r = None\n",
        "    try:\n",
        "      r = requests.get(url, headers=headers)\n",
        "      self.metrics.increment_counter(COUNTER_URLS_VISITED)\n",
        "    except:\n",
        "      self.logger.log(\"[ERROR] cannot request {}\".format(url))\n",
        "      self.metrics.increment_counter(COUNTER_FAILED_REQUESTS)\n",
        "    return r\n",
        "\n",
        "  def request_image(self, url):\n",
        "    r = None\n",
        "    try:\n",
        "      r = requests.get(url, headers=headers, stream=True)\n",
        "      self.metrics.increment_counter(COUNTER_URLS_VISITED)\n",
        "    except:\n",
        "      self.logger.log(\"[ERROR] cannot request image {}\".format(url))\n",
        "      self.metrics.increment_counter(COUNTER_FAILED_REQUESTS)\n",
        "    return r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81aHy97-EjYb"
      },
      "source": [
        "## RequestsQueue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpklTxq9EphE"
      },
      "source": [
        "from collections import deque\n",
        "# from UrlManager import UrlManager\n",
        "# from Logger import Logger\n",
        "# from Metrics import *\n",
        "\n",
        "# MAX_DEPTH = 1\n",
        "\n",
        "class RequestsQueue:\n",
        "  def __init__(self, root_url):\n",
        "    self.__deque = deque()\n",
        "    self.__urls_visited = set()\n",
        "    self.__root_url = root_url\n",
        "    self.logger_qs = Logger('RequestsQueue', 'query_strings')\n",
        "    self.logger_domain = Logger('RequestsQueue', 'different_domain')\n",
        "    self.metrics = Metrics.instance()\n",
        "    self.current_depth = 0\n",
        "    self.current_depth_counter = 0\n",
        "  \n",
        "  def add(self, url, depth):\n",
        "    if not UrlManager.is_same_domain(self.__root_url, url):\n",
        "      self.metrics.increment_counter(COUNTER_DIFFERENT_DOMAINS_AVOIDED)\n",
        "      self.logger_domain.log(\"[INFO] url with domain {} was ignored\".format(UrlManager.get_domain(url)))\n",
        "      return\n",
        "    if UrlManager.has_query_strings(url):\n",
        "      self.metrics.increment_counter(COUNTER_QUERY_STRINGS_AVOIDED)\n",
        "      self.logger_qs.log(\"[INFO] url with query strings was ignored: {}\".format(url))\n",
        "      return\n",
        "    if depth >= MAX_DEPTH:\n",
        "      return\n",
        "    if url not in self.__urls_visited:\n",
        "      self.__deque.append((url, depth))\n",
        "      self.__urls_visited.add(url)\n",
        "    self.metrics.set_gauge(GAUGE_QUEUE_LENGTH, len(self.__deque))\n",
        "  \n",
        "  def get(self):\n",
        "    if len(self.__deque):\n",
        "      next_url = self.__deque.popleft()\n",
        "      self.metrics.set_gauge(GAUGE_QUEUE_LENGTH, len(self.__deque))\n",
        "      if next_url[1] != self.current_depth:\n",
        "        print(\"depth {} finished: {} urls\".format(self.current_depth, self.current_depth_counter))\n",
        "        self.current_depth = next_url[1]\n",
        "        self.current_depth_counter = 1\n",
        "        return next_url\n",
        "      self.current_depth_counter += 1\n",
        "      return next_url\n",
        "    return None\n",
        "\n",
        "  @property\n",
        "  def is_queue_empty(self):\n",
        "    return len(self.__deque) == 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0TkCON0EyrT"
      },
      "source": [
        "## main\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av9LpF04E0D7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b7ca5501-2d57-4246-a7cc-19c98cc772f2"
      },
      "source": [
        "# import requests\n",
        "# import FileManager\n",
        "# from FileManager import FileType\n",
        "# from RequestsQueue import RequestsQueue\n",
        "# from RequestsManager import RequestsManager\n",
        "# import ParseManager as parser\n",
        "# from UrlManager import UrlManager\n",
        "# from Metrics import *\n",
        "\n",
        "req = RequestsManager(ROOT_URL)\n",
        "a = FileManager(ROOT_URL, ROOT_DIR)\n",
        "queue = RequestsQueue(ROOT_URL)\n",
        "queue.add(ROOT_URL, 0)\n",
        "\n",
        "metrics = Metrics.instance()\n",
        "\n",
        "def handle_imported_file_img(response):\n",
        "  image_urls = ParseManager.get_images_urls(response)\n",
        "  for image_url in image_urls:\n",
        "    try:\n",
        "      image_stream = req.request_image(image_url)\n",
        "      a.save_file(image_url, image_stream, FileType.IMAGE)\n",
        "    except:\n",
        "      print(\"[LOG] error saving/requesting img file\")\n",
        "\n",
        "def handle_imported_file_js(response):\n",
        "  links = ParseManager.get_js_urls(response)\n",
        "  for link in links:\n",
        "    try:\n",
        "      js_file = req.simple_request(link)\n",
        "      a.save_file(link, js_file, FileType.JS)\n",
        "    except:\n",
        "      print(\"[LOG] error saving/requesting js file\")\n",
        "\n",
        "def handle_imported_file_css(response):\n",
        "  links = ParseManager.get_css_urls(response)\n",
        "  for link in links:\n",
        "    try:\n",
        "      css_file = req.simple_request(link)\n",
        "      a.save_file(link, css_file, FileType.CSS)\n",
        "    except:\n",
        "      print(\"[LOG] error saving/requesting css file\")\n",
        "\n",
        "def main():\n",
        "  metrics.start_timer()\n",
        "  while not queue.is_queue_empty:\n",
        "    url, depth = queue.get()\n",
        "    response = req.simple_request(url)\n",
        "    a.save_file(url, response, FileType.HTML)\n",
        "    links = ParseManager.get_links_to_navigate(response)\n",
        "    links = UrlManager.sanitize_links(ROOT_URL, links)\n",
        "    for l in links:\n",
        "      queue.add(l, depth + 1)\n",
        "    \n",
        "    handle_imported_file_img(response)\n",
        "    handle_imported_file_js(response)\n",
        "    handle_imported_file_css(response)\n",
        "  \n",
        "  metrics.stop_timer()\n",
        "  metrics.show_statistics()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  try:\n",
        "    main()\n",
        "  except (KeyboardInterrupt):\n",
        "    print(\"Finished with Ctrl+C\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "depth 0 finished: 1 urls\n",
            "depth 1 finished: 76 urls\n",
            "depth 2 finished: 21 urls\n",
            "depth 3 finished: 9 urls\n",
            "Running time:  128.2797451019287\n",
            "COUNTER_URLS_VISITED: 1206\n",
            "COUNTER_HTML_DOWNLOADED: 110\n",
            "COUNTER_QUERY_STRINGS_AVOIDED: 1010\n",
            "COUNTER_DIFFERENT_DOMAINS_AVOIDED: 495\n",
            "COUNTER_FAILED_REQUESTS: 62\n",
            "COUNTER_IMAGES_DOWNLOADED: 363\n",
            "COUNTER_CSS_DOWNLOADED: 21\n",
            "COUNTER_JS_DOWNLOADED: 1\n",
            "GAUGE_QUEUE_LENGTH: min(0) max(79) curr(0)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}